{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Pipeline v0.9\n",
    "This code is designed to load, aggregate, map, combine and process equivalent datasets from across previous censuses (those presently available via the Nomis API). This includes:\n",
    "\n",
    "1. Downloading datasets from Nomis (and concatenating these from multiple requests where necessary).\n",
    "2. Aggregating data from Output Areas to higher geograpies, if required.\n",
    "3. Normalising these datasets by mapping variables to common categories (eg. 10 year age bands).\n",
    "4. Creating derived \"proxy\" datasets for indicators like population density (= population / area).\n",
    "5. Combining datasets from across multiple censuses, and calculating change-over-time, % breakdowns and area rankings.\n",
    "6. Generating individual JSON files for each area with \n",
    "\n",
    "The output is flat data files suitable for a variety of data visualisation and robo-journalism applications, with a particular focus on showing change-over-time between censuses, and ranking different geographic areas based on a variety of indicators.\n",
    "\n",
    "### Caveats\n",
    "Note that the data generated by this pipeline shouldn't be used in production until data accuracy and comparability issues have been overcome. Some important caveats of the present (v2.2) code include:\n",
    "\n",
    "* Values for median age not accurate. They are just approximations based on 10 year age bands.\n",
    "* Not all of the variables here are truly comparable across censuses (eg. general health questions changed from 2001 to 2011).\n",
    "\n",
    "### Feature backlog\n",
    "\n",
    "* Add years to geography codes in filenames, to account for changes.\n",
    "* Add automatic aggregation for regions, e&w?\n",
    "* Add dummy geographies representing 10% quantiles (deciles).\n",
    "* Add JSON formats for quantiles + aggregates.\n",
    "* Come up with a function for accurately calculating median age from single-year age data.\n",
    "* Re-factor code to deal with changing geographies that can't be best-fit aggregated (eg. Output Areas, where some are merged, split, or otherwise modified). The aim would be to show change over time for the codes that remain the same, and to show only the most recent census data for the ones are new (and probably to ignore the ones that are obsolete)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "import simplejson as json\n",
    "from io import StringIO\n",
    "import pyarrow as pa # Install pyarrow with PIP not Conda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running this code\n",
    "**Notes on config:** The idea is to have a pipeline that can load multiple datasets from multiple APIs, and at multiple geographic levels. The present config structure allows for multiple datasets, but only from a single API (Nomis). Also, at the moment some of the geographic lookups (for names, areas in hectares and parent geographies) are hard-coded in CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG FOR LOADING AND PROCESSING DATA\n",
    "SETTINGS = {\n",
    "    'geocodes': {\n",
    "        'source': {\n",
    "            '2001': {\n",
    "                'geography': 'lad', # Output Areas (code needs to match lookup code - see LOOKUPS config)\n",
    "                'code': 'TYPE464', # 2001 Nomis type code\n",
    "                'year': '2015', # Reference year for geography (needs to match lookup year - see LOOKUPS config)\n",
    "                'count': 348 # Number of geographies of this type (required for paginating API requests to Nomis)\n",
    "            },\n",
    "            '2011': {\n",
    "                'geography': 'lad',\n",
    "                'code': 'TYPE464',\n",
    "                'year': '2015',\n",
    "                'count': 348\n",
    "            }\n",
    "        },\n",
    "        'output': {\n",
    "            'geography': 'lad', # Districts (code needs to match lookup, generated above)\n",
    "            'year': '2021' # This is the year associated with the geographic boundaries, not the data\n",
    "        }\n",
    "    },\n",
    "    'cols': ['geocode','year','code','value'],\n",
    "    'apiurl': 'http://www.nomisweb.co.uk/api/v01/dataset/{dataset}.data.csv?date=latest&geography={geocode}&{cell}={cells}&measures=20100&select={selection}&uid={key}&recordlimit={limit}&recordoffset={offset}{query_misc}',\n",
    "    'apikey': '0x3cfb19ead752b37bb90da0eb3a0fe78baa9fa055',\n",
    "    'filepath': '{path}/{code}_{geography}_{suffix}.parquet',\n",
    "    'jsonpath': '{path}/{code}{suffix}.json',\n",
    "    'flatpath': '{path}/{geography}_{filename}.csv',\n",
    "    'parent': 'lookup/{child}{cyear}_{parent}{pyear}.csv', # Child > parent lookup for data aggregations\n",
    "    'lookup': 'lookup/code_lookup.csv' # Lookup file for names, parents, areas, bounds\n",
    "}\n",
    "\n",
    "DATASETS = [\n",
    "    {\n",
    "        'code': 'population',\n",
    "        'name': 'Population, male/female',\n",
    "        'sources': [\n",
    "            {\n",
    "                'year': '2001',\n",
    "                'dataset': 'NM_1634_1', # This is the table code for the Nomis API\n",
    "                'table': 'KS001', # This is the official ONS table code (not actually used in this app)\n",
    "                'geocode': SETTINGS['geocodes']['source']['2001']['code'],\n",
    "                'rowcount': SETTINGS['geocodes']['source']['2001']['count'],\n",
    "                'cell': 'cell', # This is the cell that contains the observation code on Nomis\n",
    "                'cells': [1,2], # These are the selected observation codes\n",
    "                'cellnames': ['male','female'], # These are human readable names for the above observation codes\n",
    "                'selection': ['geography_code','date_name','cell','obs_value'] # This is the selection of columns to load from Nomis\n",
    "            },\n",
    "            {\n",
    "                'year': '2011',\n",
    "                'dataset': 'NM_144_1',\n",
    "                'table': 'KS101EW',\n",
    "                'geocode': SETTINGS['geocodes']['source']['2011']['code'],\n",
    "                'rowcount': SETTINGS['geocodes']['source']['2011']['count'],\n",
    "                'cell': 'cell',\n",
    "                'cells': [1,2],\n",
    "                'cellnames': ['male','female'],\n",
    "                'selection': ['geography_code','date_name','cell','obs_value'],\n",
    "                'query_misc': '&rural_urban=0'\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'code': 'age',\n",
    "        'name': 'Age, 1 year',\n",
    "        'exclude': True, # Exclude this dataset from ALL_DATASETS\n",
    "        'sources': [\n",
    "            {\n",
    "                'year': '2001',\n",
    "                'dataset': 'NM_1637_1',\n",
    "                'table': 'UV004',\n",
    "                'geocode': SETTINGS['geocodes']['source']['2001']['code'],\n",
    "                'rowcount': SETTINGS['geocodes']['source']['2001']['count'],\n",
    "                'cell': 'c_age',\n",
    "                'cells': list(range(1,82)),\n",
    "                'cellnames': [\n",
    "                    '0','1','2','3','4','5','6','7','8','9',\n",
    "                    '10','11','12','13','14','15','16','17','18','19',\n",
    "                    '20','21','22','23','24','25','26','27','28','29',\n",
    "                    '30','31','32','33','34','35','36','37','38','39',\n",
    "                    '40','41','42','43','44','45','46','47','48','49',\n",
    "                    '50','51','52','53','54','55','56','57','58','59',\n",
    "                    '60','61','62','63','64','65','66','67','68','69',\n",
    "                    '70','71','72','73','74',\n",
    "                    '75-79','80-84','85-89','90-94','95-99','100plus'\n",
    "                ],\n",
    "                'selection': ['geography_code','date_name','c_age','obs_value']\n",
    "            },\n",
    "            {\n",
    "                'year': '2011',\n",
    "                'dataset': 'NM_503_1',\n",
    "                'table': 'QS103EW',\n",
    "                'geocode': SETTINGS['geocodes']['source']['2011']['code'],\n",
    "                'rowcount': SETTINGS['geocodes']['source']['2011']['count'],\n",
    "                'cell': 'c_age',\n",
    "                'cells': list(range(1,102)),\n",
    "                'cellnames': [\n",
    "                    '0','1','2','3','4','5','6','7','8','9',\n",
    "                    '10','11','12','13','14','15','16','17','18','19',\n",
    "                    '20','21','22','23','24','25','26','27','28','29',\n",
    "                    '30','31','32','33','34','35','36','37','38','39',\n",
    "                    '40','41','42','43','44','45','46','47','48','49',\n",
    "                    '50','51','52','53','54','55','56','57','58','59',\n",
    "                    '60','61','62','63','64','65','66','67','68','69',\n",
    "                    '70','71','72','73','74','75','76','77','78','79',\n",
    "                    '80','81','82','83','84','85','86','87','88','89',\n",
    "                    '90','91','92','93','94','95','96','97','98','99',\n",
    "                    '100plus'\n",
    "                ],\n",
    "                'cellmaps': {\n",
    "                    '75-79':['75','76','77','78','79'],\n",
    "                    '80-84':['80','81','82','83','84'],\n",
    "                    '85-89':['85','86','87','88','89'],\n",
    "                    '90-94':['90','91','92','93','94'],\n",
    "                    '95-99':['95','96','97','98','99']\n",
    "                },\n",
    "                'selection': ['geography_code','date_name','c_age','obs_value'],\n",
    "                'query_misc': '&rural_urban=0'\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'code': 'ethnicity',\n",
    "        'name': 'Ethnicity, 5 categories',\n",
    "        'sources': [\n",
    "            {\n",
    "                'year': '2001',\n",
    "                'dataset': 'NM_1606_1',\n",
    "                'table': 'KS006',\n",
    "                'geocode': SETTINGS['geocodes']['source']['2001']['code'],\n",
    "                'rowcount': SETTINGS['geocodes']['source']['2001']['count'],\n",
    "                'cell': 'c_ethpuk11',\n",
    "                'cells': [100,200,8,9,10,11,400,15,16],\n",
    "                'cellnames': ['white','mixed','indian','pakistani','bangladeshi','asian_other','black','chinese','other'],\n",
    "                'cellmaps': {\n",
    "                    'asian': ['indian','pakistani','bangladeshi','asian_other','chinese']\n",
    "                },\n",
    "                'selection': ['geography_code','date_name','c_ethpuk11','obs_value']\n",
    "            },\n",
    "            {\n",
    "                'year': '2011',\n",
    "                'dataset': 'NM_608_1',\n",
    "                'table': 'KS201EW',\n",
    "                'geocode': SETTINGS['geocodes']['source']['2011']['code'],\n",
    "                'rowcount': SETTINGS['geocodes']['source']['2011']['count'],\n",
    "                'cell': 'cell',\n",
    "                'cells': [100,200,300,400,500],\n",
    "                'cellnames': ['white','mixed','asian','black','other'],\n",
    "                'selection': ['geography_code','date_name','cell','obs_value'],\n",
    "                'query_misc': '&rural_urban=0'\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'code': 'health',\n",
    "        'name': 'Health',\n",
    "        'sources': [\n",
    "            {\n",
    "                'year': '2001',\n",
    "                'dataset': 'NM_1645_1',\n",
    "                'table': 'UV020',\n",
    "                'geocode': SETTINGS['geocodes']['source']['2001']['code'],\n",
    "                'rowcount': SETTINGS['geocodes']['source']['2001']['count'],\n",
    "                'cell': 'c_health',\n",
    "                'cells': [1,2,3],\n",
    "                'cellnames': ['good','fair','bad'],\n",
    "                'selection': ['geography_code','date_name','c_health','obs_value']\n",
    "            },\n",
    "            {\n",
    "                'year': '2011',\n",
    "                'dataset': 'NM_531_1',\n",
    "                'table': 'QS302EW',\n",
    "                'geocode': SETTINGS['geocodes']['source']['2011']['code'],\n",
    "                'rowcount': SETTINGS['geocodes']['source']['2011']['count'],\n",
    "                'cell': 'c_health',\n",
    "                'cells': [1,2,3,4,5],\n",
    "                'cellnames': ['very_good','good','fair','bad','very_bad'],\n",
    "                'cellmaps': {\n",
    "                    'good':['very_good','good'],\n",
    "                    'bad':['bad','very_bad']\n",
    "                },\n",
    "                'selection': ['geography_code','date_name','c_health','obs_value'],\n",
    "                'query_misc': '&rural_urban=0'\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'code': 'economic',\n",
    "        'name': 'Economic Activity',\n",
    "        'sources': [\n",
    "            {\n",
    "                'year': '2001',\n",
    "                'dataset': 'NM_1651_1',\n",
    "                'table': 'UV028',\n",
    "                'geocode': SETTINGS['geocodes']['source']['2001']['code'],\n",
    "                'rowcount': SETTINGS['geocodes']['source']['2001']['count'],\n",
    "                'cell': 'c_ecopuk11',\n",
    "                'cells': [2,5,8,11,12,13],\n",
    "                'cellnames': ['employee','self-employed_employees','self-employed','unemployed','student','inactive'],\n",
    "                'cellmaps': {\n",
    "                    'self-employed':['self-employed_employees','self-employed']\n",
    "                },\n",
    "                'selection': ['geography_code','date_name','c_ecopuk11','obs_value']\n",
    "            },\n",
    "            {\n",
    "                'year': '2011',\n",
    "                'dataset': 'NM_556_1',\n",
    "                'table': 'QS601EW',\n",
    "                'geocode': SETTINGS['geocodes']['source']['2011']['code'],\n",
    "                'rowcount': SETTINGS['geocodes']['source']['2011']['count'],\n",
    "                'cell': 'cell',\n",
    "                'cells': [2,3,4,5,6,7,8,9,10],\n",
    "                'cellnames': ['employee_pt','employee_ft','self-employed_employees_pt','self-employed_employees_ft','self-employed_pt','self-employed_ft','unemployed','student','inactive'],\n",
    "                'cellmaps': {\n",
    "                    'employee':['employee_pt','employee_ft'],\n",
    "                    'self-employed':['self-employed_employees_pt','self-employed_employees_ft','self-employed_pt','self-employed_ft']\n",
    "                },\n",
    "                'selection': ['geography_code','date_name','cell','obs_value'],\n",
    "                'query_misc': '&rural_urban=0'\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'code': 'travel',\n",
    "        'name': 'Travel to Work',\n",
    "        'sources': [\n",
    "            {\n",
    "                'year': '2001',\n",
    "                'dataset': 'NM_1659_1',\n",
    "                'table': 'UV037',\n",
    "                'geocode': SETTINGS['geocodes']['source']['2001']['code'],\n",
    "                'rowcount': SETTINGS['geocodes']['source']['2001']['count'],\n",
    "                'cell': 'transport_powpew11',\n",
    "                'cells': [1,2,3,4,5,6,7,8,9,10,11],\n",
    "                'cellnames': ['home','metro','train','bus','taxi','car_van','car_van_passenger','moto','bicycle','foot','other'],\n",
    "                'cellmaps': {\n",
    "                    'train_metro':['metro','train'],\n",
    "                    'car_van':['car_van','car_van_passenger']\n",
    "                },\n",
    "                'selection': ['geography_code','date_name','transport_powpew11','obs_value']\n",
    "            },\n",
    "            {\n",
    "                'year': '2011',\n",
    "                'dataset': 'NM_568_1',\n",
    "                'table': 'QS703EW',\n",
    "                'geocode': SETTINGS['geocodes']['source']['2011']['code'],\n",
    "                'rowcount': SETTINGS['geocodes']['source']['2011']['count'],\n",
    "                'cell': 'cell',\n",
    "                'cells': [1,2,3,4,5,6,7,8,9,10,11],\n",
    "                'cellnames': ['home','metro','train','bus','taxi','moto','car_van','car_van_passenger','bicycle','foot','other'],\n",
    "                'cellmaps': {\n",
    "                    'train_metro':['metro','train'],\n",
    "                    'car_van':['car_van','car_van_passenger']\n",
    "                },\n",
    "                'selection': ['geography_code','date_name','cell','obs_value'],\n",
    "                'query_misc': '&rural_urban=0'\n",
    "            },\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'code': 'tenure',\n",
    "        'name': 'Housing Tenure',\n",
    "        'sources': [\n",
    "            {\n",
    "                'year': '2001',\n",
    "                'dataset': 'NM_1680_1',\n",
    "                'table': 'UV063',\n",
    "                'geocode': SETTINGS['geocodes']['source']['2001']['code'],\n",
    "                'rowcount': SETTINGS['geocodes']['source']['2001']['count'],\n",
    "                'cell': 'c_tenhuk11',\n",
    "                'cells': [1,4,5,8,13],\n",
    "                'cellnames': ['owned','shared_ownership','rented_social','rented_private','rent_free'],\n",
    "                'selection': ['geography_code','date_name','c_tenhuk11','obs_value']\n",
    "            },\n",
    "            {\n",
    "                'year': '2011',\n",
    "                'dataset': 'NM_537_1',\n",
    "                'table': 'QS405EW',\n",
    "                'geocode': SETTINGS['geocodes']['source']['2011']['code'],\n",
    "                'rowcount': SETTINGS['geocodes']['source']['2011']['count'],\n",
    "                'cell': 'c_tenhuk11',\n",
    "                'cells': [1,4,5,8,13],\n",
    "                'cellnames': ['owned','shared_ownership','rented_social','rented_private','rent_free'],\n",
    "                'selection': ['geography_code','date_name','c_tenhuk11','obs_value'],\n",
    "                'query_misc': '&rural_urban=0'\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "PROXY_DATASETS = [\n",
    "    {\n",
    "        'code': 'density',\n",
    "        'name': 'Population Density',\n",
    "        'source': 'population'\n",
    "    },\n",
    "    {\n",
    "        'code': 'age10yr',\n",
    "        'name': 'Age, 10 year',\n",
    "        'source': 'age',\n",
    "        'cellmaps': {\n",
    "            '0-9':['0','1','2','3','4','5','6','7','8','9'],\n",
    "            '10-19':['10','11','12','13','14','15','16','17','18','19'],\n",
    "            '20-29':['20','21','22','23','24','25','26','27','28','29'],\n",
    "            '30-39':['30','31','32','33','34','35','36','37','38','39'],\n",
    "            '40-49':['40','41','42','43','44','45','46','47','48','49'],\n",
    "            '50-59':['50','51','52','53','54','55','56','57','58','59'],\n",
    "            '60-69':['60','61','62','63','64','65','66','67','68','69'],\n",
    "            '70-79':['70','71','72','73','74',],\n",
    "            '80plus':['75-79','80-84','85-89','90-94','95-99','100plus']\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'code': 'agemed',\n",
    "        'name': 'Median Age',\n",
    "        'source': 'age'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Combined list of standard and proxy datasets (excluding age by single year)\n",
    "ALL_DATASETS = [d for d in DATASETS if 'exclude' not in d] + PROXY_DATASETS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "All of the functions are defined in the cell below. Note that the functions are currently called sequentially in the later cells to build a single dataset, but could be wrapped in another function in order to load multiple datasets at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION B. FUNCTIONS\n",
    "\n",
    "# Get all raw sources for a dataset\n",
    "def get_sources(dataset, settings):\n",
    "    code = dataset['code']\n",
    "    cols = settings['cols']\n",
    "    \n",
    "    for source in dataset['sources']:\n",
    "        get_raw(code, cols, source, settings)\n",
    "\n",
    "# Load a data source from an API to a Parquet file\n",
    "def get_raw(code, cols, source, settings, override=False):\n",
    "    year = source['year']\n",
    "    geography = settings['geocodes']['source'][source['year']]['geography']\n",
    "    geography_yr = settings['geocodes']['source'][source['year']]['year']\n",
    "    path = 'raw'\n",
    "    \n",
    "    # Set save path\n",
    "    save_path = settings['filepath'].format(\n",
    "        path=path,\n",
    "        code=code,\n",
    "        geography=geography + geography_yr,\n",
    "        suffix=str(source['year'])\n",
    "    )\n",
    "    \n",
    "    if not os.path.exists(save_path) or override == True:\n",
    "        # Split API call into maximum 250,000 cell chunks\n",
    "        rowcount = source['rowcount'] * len(source['cells'])\n",
    "        maxrows = 250000\n",
    "        offsets = []\n",
    "        offset = 0\n",
    "        while offset < rowcount:\n",
    "            offsets.append(offset)\n",
    "            offset += maxrows\n",
    "    \n",
    "        dfs = []\n",
    "    \n",
    "        for i, offset in enumerate(offsets, start=1):\n",
    "            # Format API url\n",
    "            if 'query_misc' in source:\n",
    "                query_misc = source['query_misc']\n",
    "            else:\n",
    "                query_misc = ''\n",
    "        \n",
    "            url = settings['apiurl'].format(\n",
    "                dataset=source['dataset'],\n",
    "                geocode=source['geocode'],\n",
    "                cell=source['cell'],\n",
    "                cells=','.join(str(x) for x in source['cells']),\n",
    "                selection=','.join(source['selection']),\n",
    "                key=settings['apikey'],\n",
    "                limit=maxrows,\n",
    "                offset=offset,\n",
    "                query_misc=query_misc\n",
    "            )\n",
    "            \n",
    "            # Read CSV from API\n",
    "            save_path_part = save_path + '.part' + str(i)\n",
    "            df = None\n",
    "            \n",
    "            if not os.path.exists(save_path_part):\n",
    "                print(url)\n",
    "                response = requests.get(url)\n",
    "                \n",
    "                # Convert CSV string to DataFrame + save to Parquet\n",
    "                df = pd.read_csv(StringIO(response.text))\n",
    "                df.to_parquet(save_path_part, index=False)\n",
    "            else:\n",
    "                df = pd.read_parquet(save_path_part)\n",
    "            \n",
    "            print('Loaded ' + code + ' ' + year + ' segment ' + str(i) + '/' + str(len(offsets)))\n",
    "        \n",
    "            # Add dataframe to stack\n",
    "            dfs.append(df)\n",
    "    \n",
    "        # Combine loaded dataframes in stack\n",
    "        df = pd.concat(dfs, ignore_index=True)\n",
    "        \n",
    "        # Rename columns\n",
    "        colmap = {}\n",
    "        for i in range(len(cols)):\n",
    "            colmap[source['selection'][i].upper()] = cols[i]\n",
    "        df = df.rename(columns=colmap)\n",
    "        \n",
    "        # Rename cells\n",
    "        for i in range(len(source['cells'])):\n",
    "            df.loc[df['code'] == source['cells'][i], 'code'] = source['cellnames'][i]\n",
    "            \n",
    "        # Map cells (if map exists)\n",
    "        if 'cellmaps' in source:\n",
    "            df = map_raw(df, source['cellmaps'], cols)\n",
    "\n",
    "        # Create output directory if needed\n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "    \n",
    "        # Save DataFrame as Parquet file\n",
    "        print(df)\n",
    "        df.to_parquet(save_path, index=False)\n",
    "        print('Combined + mapped ' + save_path)\n",
    "        \n",
    "        # Remove Parquet temporary (part) files\n",
    "        for i, offset in enumerate(offsets, start=1):\n",
    "            save_path_part = save_path + '.part' + str(i)\n",
    "            # os.remove(save_path_part)\n",
    "        \n",
    "    else:\n",
    "        print('Already exists ' + save_path)\n",
    "\n",
    "# Combine (sum) variables using mapping\n",
    "def map_raw(df, cellmaps, cols):\n",
    "    # Rename cells in code column using cell mapping\n",
    "    for key in cellmaps:\n",
    "        for code in cellmaps[key]:\n",
    "            df.loc[df['code'] == code, 'code'] = key\n",
    "    \n",
    "    # Group by code column\n",
    "    groups = cols[:len(cols) - 1]\n",
    "    df = df.groupby(groups, as_index=False).sum()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Group (aggregate) data using lookups\n",
    "def group_data(dataset, settings):\n",
    "    code = dataset['code']\n",
    "    cols = settings['cols']\n",
    "    path = 'raw'\n",
    "    geography = settings['geocodes']['output']['geography']\n",
    "    geography_yr = settings['geocodes']['output']['year']\n",
    "    \n",
    "    for source in dataset['sources']:\n",
    "        source_geo = settings['geocodes']['source'][source['year']]['geography']\n",
    "        source_geo_yr = settings['geocodes']['source'][source['year']]['year']\n",
    "        \n",
    "        if source_geo + source_geo_yr != geography + geography_yr:\n",
    "            load_path = settings['filepath'].format(\n",
    "                path=path,\n",
    "                code=code,\n",
    "                geography=source_geo + source_geo_yr,\n",
    "                suffix=str(source['year'])\n",
    "            )\n",
    "            lookup_path = settings['parent'].format(\n",
    "                child=source_geo,\n",
    "                cyear=source_geo_yr,\n",
    "                parent=geography,\n",
    "                pyear=geography_yr\n",
    "            )\n",
    "            if not os.path.exists(lookup_path):\n",
    "                lookup_path = settings['parent'].format(\n",
    "                    child=source_geo,\n",
    "                    cyear=geography_yr,\n",
    "                    parent=geography,\n",
    "                    pyear=geography_yr\n",
    "                )\n",
    "        \n",
    "            # Load OA data and higher geography lookup for grouping\n",
    "            df = pd.read_parquet(load_path)\n",
    "            df.set_index('geocode', inplace=True)\n",
    "            lookup = pd.read_csv(lookup_path)\n",
    "            lookup.set_index('code', inplace=True)\n",
    "        \n",
    "            # Merge the datasets on the OA code, then drop the OA column\n",
    "            df = df.merge(lookup, left_index=True, right_index=True)\n",
    "            df = df.reset_index(drop=True)\n",
    "        \n",
    "            # Clean up the columns\n",
    "            df = df.rename(columns={'parent': 'geocode'})\n",
    "            df = df[cols]\n",
    "        \n",
    "            # Group (aggregate) the data up to the higher geography\n",
    "            df = df.groupby(cols[:-1], as_index=False).sum()\n",
    "        \n",
    "            # Set the filepath and save the aggregated dataset\n",
    "            save_path = settings['filepath'].format(\n",
    "                path=path,\n",
    "                code=code,\n",
    "                geography=geography + geography_yr,\n",
    "                suffix=str(source['year'])\n",
    "            )\n",
    "            df.to_parquet(save_path, index=False)\n",
    "            print('Aggregated ' + save_path)\n",
    "\n",
    "# Combine raw Parquet data source files into a single Tidy Data file\n",
    "def combine_data(dataset, settings):\n",
    "    in_path = 'raw'\n",
    "    out_path = 'processed'\n",
    "    suffix = 'tidy'\n",
    "    geography = settings['geocodes']['output']['geography']\n",
    "    geography_yr = settings['geocodes']['output']['year']\n",
    "    code = dataset['code']\n",
    "    cols = settings['cols']\n",
    "    \n",
    "    # Load raw datasets into an array\n",
    "    datasets = []\n",
    "    for source in dataset['sources']:\n",
    "        load_path = settings['filepath'].format(\n",
    "            path=in_path,\n",
    "            code=code,\n",
    "            geography=geography + geography_yr,\n",
    "            suffix=str(source['year'])\n",
    "        )\n",
    "        df = pd.read_parquet(load_path)\n",
    "        datasets.append(df)\n",
    "    \n",
    "    # Combine datasets from array\n",
    "    combined = pd.concat(datasets)\n",
    "    \n",
    "    # Sort rows\n",
    "    combined = combined.sort_values(cols[:-1], ascending=True)\n",
    "    \n",
    "    # Create output directory if needed\n",
    "    if not os.path.exists(out_path):\n",
    "        os.mkdir(out_path)\n",
    "    \n",
    "    # Save combined tidydata file\n",
    "    save_path = settings['filepath'].format(\n",
    "        path=out_path,\n",
    "        code=code,\n",
    "        geography=geography + geography_yr,\n",
    "        suffix=suffix\n",
    "    )\n",
    "    combined.to_parquet(save_path, index=False)\n",
    "    print('Combined ' + save_path)\n",
    "\n",
    "# Pivot data\n",
    "def pivot_data(dataset, settings):\n",
    "    cols = settings['cols']\n",
    "    code = dataset['code']\n",
    "    path = 'processed'\n",
    "    geography = settings['geocodes']['output']['geography']\n",
    "    geography_yr = settings['geocodes']['output']['year']\n",
    "    \n",
    "    # Load tidy data Parquet file as dataframe\n",
    "    load_path = settings['filepath'].format(\n",
    "        path=path,\n",
    "        code=code,\n",
    "        geography=geography + geography_yr,\n",
    "        suffix='tidy'\n",
    "    )\n",
    "    df = pd.read_parquet(load_path)\n",
    "    \n",
    "    # Pivot the data\n",
    "    pivot = df.pivot_table(index=cols[0], columns=cols[1:len(cols) - 1], values='value', fill_value=0)\n",
    "    \n",
    "    # Add additional 'value' level to index\n",
    "    pivot.columns = pd.MultiIndex.from_product([['value']] + pivot.columns.levels)\n",
    "    \n",
    "    # Replace empty cells with 0 values (deals with new areas that didn't have data for earlier censuses)\n",
    "    pivot = pivot.fillna(np.nan, downcast='infer')\n",
    "    \n",
    "    # Create output directory if needed\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    \n",
    "    # Save pivoted file\n",
    "    save_path = settings['filepath'].format(\n",
    "        path=path,\n",
    "        code=code,\n",
    "        geography=geography + geography_yr,\n",
    "        suffix='pivot'\n",
    "    )\n",
    "    \n",
    "    # Use pyarrow to write multiindex parquet file\n",
    "    pivot = pa.Table.from_pandas(pivot)\n",
    "    pa.parquet.write_table(pivot, save_path)\n",
    "    \n",
    "    # pivot.to_parquet(save_path)\n",
    "    print('Data pivoted ' + save_path)\n",
    "\n",
    "# Add proxy datasets\n",
    "def add_proxy_data(proxy_datasets, settings):\n",
    "    path = 'processed'\n",
    "    geography = settings['geocodes']['output']['geography']\n",
    "    geography_yr = settings['geocodes']['output']['year']\n",
    "    suffix = 'pivot'\n",
    "    \n",
    "    for dataset in proxy_datasets:\n",
    "        code = dataset['code']\n",
    "        cols = settings['cols']\n",
    "        source = dataset['source']\n",
    "        \n",
    "        df = None\n",
    "        \n",
    "        # Build density dataset\n",
    "        if code == 'density':\n",
    "            print('Calculating density')\n",
    "            \n",
    "            # Set data load path\n",
    "            load_path = settings['filepath'].format(\n",
    "                path=path,\n",
    "                code=source,\n",
    "                geography=geography + geography_yr,\n",
    "                suffix='pivot'\n",
    "            )\n",
    "            # Check if data exists in directory\n",
    "            if os.path.exists(load_path):\n",
    "                df = pd.read_parquet(load_path)\n",
    "            \n",
    "                # Build dataset\n",
    "                df = add_proxy_density(df, 'density', dataset, settings)\n",
    "            \n",
    "        # Build median age dataset\n",
    "        elif code == 'agemed':\n",
    "            print('Calculating median age')\n",
    "            \n",
    "            # Set data load path\n",
    "            load_path = settings['filepath'].format(\n",
    "                path=path,\n",
    "                code=source,\n",
    "                geography=geography + geography_yr,\n",
    "                suffix='pivot'\n",
    "            )\n",
    "            # Check if data exists in directory\n",
    "            if os.path.exists(load_path):\n",
    "                df = pd.read_parquet(load_path)\n",
    "            \n",
    "                # Build datasets\n",
    "                df = add_proxy_agemed(df, 'agemed', dataset, settings)\n",
    "                \n",
    "        # Build any mapped dataset\n",
    "        elif dataset['cellmaps']:\n",
    "            print('Mapping ' + code)\n",
    "            \n",
    "            # Load tidy data file\n",
    "            load_path = settings['filepath'].format(\n",
    "                path=path,\n",
    "                code=source,\n",
    "                geography=geography + geography_yr,\n",
    "                suffix='tidy'\n",
    "            )\n",
    "            # Check if data exists in directory\n",
    "            if os.path.exists(load_path):\n",
    "                df = pd.read_parquet(load_path)\n",
    "                \n",
    "                # Map data\n",
    "                df = map_raw(df, dataset['cellmaps'], cols)\n",
    "                \n",
    "                # Pivot the data\n",
    "                df = df.pivot_table(index=cols[0], columns=cols[1:len(cols) - 1], values='value', fill_value=0)\n",
    "    \n",
    "                # Add additional 'value' level to index\n",
    "                df.columns = pd.MultiIndex.from_product([['value']] + df.columns.levels)\n",
    "    \n",
    "                # Replace empty cells with 0 values (deals with new areas that didn't have data for earlier censuses)\n",
    "                df = df.fillna(np.nan, downcast='infer')\n",
    "                \n",
    "        \n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            # Save pivoted file\n",
    "            save_path = settings['filepath'].format(\n",
    "                path=path,\n",
    "                code=code,\n",
    "                geography=geography + geography_yr,\n",
    "                suffix=suffix\n",
    "            )\n",
    "            \n",
    "            # Use pyarrow to write multiindex parquet file\n",
    "            df = pa.Table.from_pandas(df)\n",
    "            pa.parquet.write_table(df, save_path)\n",
    "            # df.to_parquet(save_path)\n",
    "            print('Proxy data ' + save_path)\n",
    "\n",
    "# Build density dataset\n",
    "def add_proxy_density(df, code, source, settings):\n",
    "    # Get existing columns\n",
    "    cols = df.columns\n",
    "    \n",
    "    # Get years\n",
    "    years = cols.unique(level='year').values.tolist()\n",
    "    \n",
    "    # Add columns for totals\n",
    "    df = add_totals(df, cols)\n",
    "    \n",
    "    # Load lookup for areas & add multi-index to allow clean merge with pivot table\n",
    "    lookup = pd.read_csv(settings['lookup'])\n",
    "    lookup.set_index('code', inplace=True)\n",
    "    areas = lookup[['area']]\n",
    "    \n",
    "    areas.columns = pd.MultiIndex.from_product([['value'], ['all'], areas.columns])\n",
    "    \n",
    "    # Add column for density\n",
    "    df = df.merge(areas, left_index=True, right_index=True)\n",
    "    df.index.name = 'geocode' # The name of the index column gets lost with the merge\n",
    "    \n",
    "    # Get new columns (and create empty list to remap names of final output columns)\n",
    "    cols = df.columns\n",
    "    \n",
    "    for year in years:\n",
    "        # Add population density column (for each year)\n",
    "        df['value', year, 'density'] = round(df['value', year, 'all'] / df['value','all','area'], 2)\n",
    "            \n",
    "    # Remove redundant columns & rename remaining\n",
    "    df = df.drop(columns=cols)\n",
    "    df = df.rename(columns={'density': 'all'}, level=2)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Build median age dataset\n",
    "def add_proxy_agemed(df, code, source, settings):\n",
    "    # Get new columns\n",
    "    cols = df.columns\n",
    "    \n",
    "    # Get census years\n",
    "    years = cols.unique(level='year').values.tolist()\n",
    "    \n",
    "    # Ages as strings\n",
    "    ages = [str(x) for x in list(range(0, 75))]\n",
    "    \n",
    "    # Add columns for totals\n",
    "    df = add_totals(df, cols)\n",
    "    \n",
    "    # Add columns for median age\n",
    "    for year in years:\n",
    "        df['value', year, 'median'] = None\n",
    "        df['value', year, 'min'] = 0\n",
    "        df['value', year, 'max'] = 0\n",
    "        df['value', year, 'half'] = df['value', year, 'all'] / 2\n",
    "        \n",
    "        for age in ages:\n",
    "            df['value', year, 'min'] = df['value', year, 'max']\n",
    "            df['value', year, 'max'] +=  df['value', year, age]\n",
    "            \n",
    "            df.loc[(df['value', year, 'half'] > df['value', year, 'min']) & (df['value', year, 'half'] <= df['value', year, 'max']), ('value', year, 'median')] = int(age)\n",
    "    \n",
    "    # Remove redundant columns & rename remaining\n",
    "    df = df[[('value', 2001, 'median'), ('value', 2011, 'median')]]\n",
    "    df = df.rename(columns={'median': 'all'}, level=2)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Add calculated columns to tidy data dataframe\n",
    "def add_calcs(dataset, settings):\n",
    "    code = dataset['code']\n",
    "    path = 'processed'\n",
    "    geography = settings['geocodes']['output']['geography']\n",
    "    geography_yr = settings['geocodes']['output']['year']\n",
    "    \n",
    "    # Load pivoted data as dataframe\n",
    "    load_path = settings['filepath'].format(\n",
    "        path=path,\n",
    "        code=code,\n",
    "        geography=geography + geography_yr,\n",
    "        suffix='pivot'\n",
    "    )\n",
    "    df = pd.read_parquet(load_path)\n",
    "    \n",
    "    # Get columns\n",
    "    cols = df.columns\n",
    "    \n",
    "    # Get codes\n",
    "    codes = cols.unique(level='code').values.tolist()\n",
    "    \n",
    "    # Add columns for totals\n",
    "    if len(codes) > 1:\n",
    "        df = add_totals(df, cols)\n",
    "    \n",
    "    # Add % change to totals since previous census\n",
    "    df = add_change(df, cols)\n",
    "    \n",
    "    # Add columns for % breakdown (incl. change since prev. census)\n",
    "    if len(codes) > 1:\n",
    "        df = add_percent(df, cols)\n",
    "    \n",
    "    # Add columns for absolute and % ranks\n",
    "    if len(df.index) > 1:\n",
    "        df = add_ranks(df)\n",
    "    \n",
    "    # Create output directory if needed\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    \n",
    "    # Save combined tidydata file\n",
    "    save_path = settings['filepath'].format(\n",
    "        path=path,\n",
    "        code=code,\n",
    "        geography=geography + geography_yr,\n",
    "        suffix='calcs'\n",
    "    )\n",
    "    df = pa.Table.from_pandas(df)\n",
    "    pa.parquet.write_table(df, save_path)\n",
    "    # df.to_parquet(save_path)\n",
    "    print('Calculated fields added ' + save_path)\n",
    "\n",
    "# Generate an indexed \"JSON\" file with quantiles for each indicator\n",
    "def make_quantiles(df, quan='deciles'):\n",
    "    \n",
    "    # Get column headers\n",
    "    cols = df.columns\n",
    "    \n",
    "    # Dictionary to hold data\n",
    "    data = {}\n",
    "    \n",
    "    for col in cols:\n",
    "        if 'rank' not in col[1]:\n",
    "            # Get values in column (excluding NaN values)\n",
    "            vals = df[col].tolist()\n",
    "            vals = [d for d in vals if not (math.isinf(d) or math.isnan(d))]\n",
    "            vals.sort()\n",
    "            count = len(vals)\n",
    "            \n",
    "            quantiles = []\n",
    "            \n",
    "            if quan == 'deciles':\n",
    "                quantiles = [\n",
    "                    vals[0],\n",
    "                    vals[math.floor(count * 0.1)],\n",
    "                    vals[math.floor(count * 0.2)],\n",
    "                    vals[math.floor(count * 0.3)],\n",
    "                    vals[math.floor(count * 0.4)],\n",
    "                    vals[math.floor(count * 0.5)],\n",
    "                    vals[math.floor(count * 0.6)],\n",
    "                    vals[math.floor(count * 0.7)],\n",
    "                    vals[math.floor(count * 0.8)],\n",
    "                    vals[math.floor(count * 0.9)],\n",
    "                    vals[-1]\n",
    "                ]\n",
    "            elif quan == 'quintiles':\n",
    "                quantiles = [\n",
    "                    vals[0],\n",
    "                    vals[math.floor(count * 0.2)],\n",
    "                    vals[math.floor(count * 0.4)],\n",
    "                    vals[math.floor(count * 0.6)],\n",
    "                    vals[math.floor(count * 0.8)],\n",
    "                    vals[-1]\n",
    "                ]\n",
    "            elif quan == 'quartiles':\n",
    "                quantiles = [\n",
    "                    vals[0],\n",
    "                    vals[math.floor(count * 0.25)],\n",
    "                    vals[math.floor(count * 0.5)],\n",
    "                    vals[math.floor(count * 0.75)],\n",
    "                    vals[-1]\n",
    "                ]\n",
    "                \n",
    "        \n",
    "            if col[0] not in data:\n",
    "                data[col[0]] = {}\n",
    "            \n",
    "            if col[1] not in data[col[0]]:\n",
    "                data[col[0]][col[1]] = {}\n",
    "            \n",
    "            if col[2] not in data[col[0]][col[1]]:\n",
    "                data[col[0]][col[1]][col[2]] = {}\n",
    "            \n",
    "            if col[3] not in data[col[0]][col[1]][col[2]]:\n",
    "                data[col[0]][col[1]][col[2]][col[3]] = quantiles\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Export data to indexed JSON format with parent geographies\n",
    "def export_data(datasets, settings):\n",
    "    in_path = 'processed'\n",
    "    out_path = 'final'\n",
    "    geography = settings['geocodes']['output']['geography']\n",
    "    geography_yr = settings['geocodes']['output']['year']\n",
    "    \n",
    "    # Load geography metadata lookup table\n",
    "    lookup = pd.read_csv(settings['lookup'])\n",
    "    lookup.set_index('code', inplace=True)\n",
    "    index = lookup.to_dict(orient='index')\n",
    "    \n",
    "    # Generate child lookup\n",
    "    child_lookup = {}\n",
    "    \n",
    "    for key in index:\n",
    "        if key not in child_lookup:\n",
    "            child_lookup[key] = []\n",
    "\n",
    "        parent = index[key]['parent']\n",
    "        if parent not in child_lookup:\n",
    "            child_lookup[parent] = []\n",
    "        \n",
    "        child_lookup[parent].append({\n",
    "            'code': key,\n",
    "            'name': index[key]['name'],\n",
    "            'type': index[key]['type']\n",
    "        })\n",
    "    print('Lookups generated')\n",
    "    \n",
    "    \n",
    "    # Create dict to contain dataframes\n",
    "    dfs = {}\n",
    "    \n",
    "    # Load and merge datasets\n",
    "    for dataset in datasets:\n",
    "        code = dataset['code']\n",
    "        \n",
    "        # Load Parquet with calculated fields as dataframe\n",
    "        load_path = settings['filepath'].format(\n",
    "            path=in_path,\n",
    "            code=code,\n",
    "            geography=geography + geography_yr,\n",
    "            suffix='calcs'\n",
    "        )\n",
    "        new_df = pd.read_parquet(load_path)\n",
    "        \n",
    "        dfs[code] = new_df\n",
    "    print('Datasets loaded')\n",
    "    \n",
    "    # Merge datasets into a single dataframe\n",
    "    df = pd.concat(dfs.values(), axis=1, keys=dfs.keys())\n",
    "    \n",
    "    # Save sump of merged datasets to Parquet file\n",
    "    save_path = settings['filepath'].format(\n",
    "        path=in_path,\n",
    "        code='all',\n",
    "        geography=geography + geography_yr,\n",
    "        suffix='merged'\n",
    "    )\n",
    "    dump = pa.Table.from_pandas(df)\n",
    "    pa.parquet.write_table(dump, save_path)\n",
    "    # df.to_parquet(save_path)\n",
    "    print('Datasets merged ' + save_path)\n",
    "    \n",
    "    # Create 'final' output directories if needed\n",
    "    paths = [out_path, out_path + '/json', out_path + '/json/quantiles', out_path + '/json/place']\n",
    "    for path in paths:\n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "    \n",
    "    # Generate deciles, quintiles and quartiles\n",
    "    quans = ['deciles', 'quintiles', 'quartiles']\n",
    "    quantiles = {\n",
    "        'deciles': None,\n",
    "        'quintiles': None,\n",
    "        'quartiles': None\n",
    "    }\n",
    "    \n",
    "    if len(df.index) > 20:\n",
    "        quantiles[quans[0]] = make_quantiles(df, quans[0])\n",
    "    if len(df.index) > 10:\n",
    "        quantiles[quans[1]] = make_quantiles(df, quans[1])\n",
    "    if len(df.index) > 8:\n",
    "        quantiles[quans[2]] = make_quantiles(df, quans[2])\n",
    "    \n",
    "    # Save 'quantiles' datasets\n",
    "    for quan in quans:\n",
    "        if not quantiles[quan] == None:\n",
    "            save_path = settings['jsonpath'].format(\n",
    "                path=paths[2],\n",
    "                code=quan + '_' + geography + geography_yr,\n",
    "                suffix=''\n",
    "            )\n",
    "    \n",
    "            with open(save_path, \"w\") as write_file:\n",
    "                json.dump(quantiles[quan], write_file, indent=2, ignore_nan=True)\n",
    "            print(quan.capitalize() + ' data saved to JSON file ' + save_path)\n",
    "    \n",
    "    # Generate individual JSON datasets for each geography\n",
    "    \n",
    "    # Get column headers\n",
    "    cols = df.columns\n",
    "    \n",
    "    # Get number of rows\n",
    "    count = len(df.index)\n",
    "    progress = 0\n",
    "    \n",
    "    # Generate JSON data files from dataframe, row by row\n",
    "    for i, row in df.iterrows():\n",
    "        progress += 1\n",
    "        if progress % 1000 == 0:\n",
    "            print('Generated JSON files for ' + str(progress) + ' of ' + str(count) + ' areas')\n",
    "        \n",
    "        entry = {\n",
    "            'code': i,\n",
    "            'name': index[i]['name'],\n",
    "            'type': settings['geocodes']['output']['geography'],\n",
    "            'area': index[i]['area'],\n",
    "            'count': int(count),\n",
    "            'parents': [],\n",
    "            'children': child_lookup[i],\n",
    "            'bounds': [\n",
    "                [round(index[i]['minx'], 5), round(index[i]['miny'], 5)],\n",
    "                [round(index[i]['maxx'], 5), round(index[i]['maxy'], 5)]\n",
    "            ],\n",
    "            'data': {}\n",
    "        }\n",
    "        \n",
    "        # Generate parent codes\n",
    "        parent_code = index[i]['parent']\n",
    "        \n",
    "        # Get parents, grandparents etc\n",
    "        while isinstance(parent_code, str):\n",
    "            parent = {\n",
    "                'code': parent_code,\n",
    "                'name': index[parent_code]['name'],\n",
    "                'type': index[parent_code]['type']\n",
    "            }\n",
    "            entry['parents'].append(parent)\n",
    "            \n",
    "            parent_code = index[parent_code]['parent']\n",
    "        \n",
    "        # Generate children\n",
    "        # children = lookup.loc[lookup['parent'] == i][['name', 'type']].reset_index(level=0)\n",
    "        # entry['children'] = children.to_dict(orient='records')\n",
    "        \n",
    "        # Generate data in indexed/nested format\n",
    "        data = {}\n",
    "        \n",
    "        for col in cols:\n",
    "            if col[0] not in data:\n",
    "                data[col[0]] = {}\n",
    "            \n",
    "            if col[1] not in data[col[0]]:\n",
    "                data[col[0]][col[1]] = {}\n",
    "            \n",
    "            if col[2] not in data[col[0]][col[1]]:\n",
    "                data[col[0]][col[1]][col[2]] = {}\n",
    "            \n",
    "            if col[3] not in data[col[0]][col[1]][col[2]]:\n",
    "                data[col[0]][col[1]][col[2]][col[3]] = row[col]\n",
    "        \n",
    "        entry['data'] = data\n",
    "        \n",
    "        # Save geography to JSON file\n",
    "        save_path = settings['jsonpath'].format(\n",
    "            path=paths[3],\n",
    "            code=entry['code'],\n",
    "            suffix=''\n",
    "        )\n",
    "        \n",
    "        with open(save_path, \"w\") as write_file:\n",
    "            json.dump(entry, write_file, indent=2)\n",
    "\n",
    "    print('Geographies saved to individual JSON files ' + out_path + '/{geocode}.json')    \n",
    "\n",
    "# Add column for totals (incl. change since prev. census)\n",
    "def add_totals(df, cols):\n",
    "    years = cols.unique(level='year').values.tolist()\n",
    "    codes = cols.unique(level='code').values.tolist()\n",
    "    \n",
    "    # Generate list of columns to sum\n",
    "    for year in years:\n",
    "        sum_cols = [];\n",
    "        for code in codes:\n",
    "            sum_cols.append(('value', year, code))\n",
    "        \n",
    "        # Sum list to new column (for each year)\n",
    "        df['value', year, 'all'] = df[sum_cols].sum(axis=1)\n",
    "        \n",
    "    # Sort columns\n",
    "    df = df.sort_index(axis=1)\n",
    "                \n",
    "    print('Totals added')\n",
    "    return df\n",
    "\n",
    "# Generate cols for change since prev. census\n",
    "def add_change(df, cols):\n",
    "    years = cols.unique(level='year').values.tolist()\n",
    "    codes = cols.unique(level='code').values.tolist()\n",
    "    \n",
    "    # Generate cols\n",
    "    if 'all' not in codes:\n",
    "        codes.append('all')\n",
    "        \n",
    "    \n",
    "    for code in codes:\n",
    "        year = years[-1]\n",
    "        year_prev = years[-2]\n",
    "        \n",
    "        df['value', 'change', code] = 'null'\n",
    "        \n",
    "        val = df['value', year, code]\n",
    "        val_prev = df['value', year_prev, code]\n",
    "        \n",
    "        df['value', 'change', code] = round((100 * (val / val_prev)) - 100, 2)\n",
    "        df['value', 'change', code] =  df['value', 'change', code].replace(np.nan, 0)\n",
    "    \n",
    "    # Sort columns\n",
    "    df = df.sort_index(axis=1)\n",
    "                \n",
    "    print('% change added')\n",
    "    return df\n",
    "\n",
    "# Add columns for % breakdown (incl. change since prev. census)\n",
    "def add_percent(df, cols):\n",
    "    years = cols.unique(level='year').values.tolist()\n",
    "    codes = cols.unique(level='code').values.tolist()\n",
    "    \n",
    "    # Add columns for totals\n",
    "    for year in years:\n",
    "        for code in codes:\n",
    "            \n",
    "            # Sum list to new column (for each year)\n",
    "            df['perc', year, code] = round(100 * (df['value', year, code] / df['value', year, 'all']), 2)\n",
    "    \n",
    "    # Generate cols for change since prev. census\n",
    "    for code in codes:\n",
    "        year = years[len(years) - 1]\n",
    "        prev_year = years[len(years) - 2]\n",
    "        df['perc', 'change', code] = round(df['perc', year, code] - df['perc', prev_year, code], 2)\n",
    "        \n",
    "    # Replace empty cells with 0.0 values (treats zero out of zero as 0.0%)\n",
    "    df = df.fillna(np.nan, downcast='infer')\n",
    "    \n",
    "    # Sort columns\n",
    "    df = df.sort_index(axis=1)\n",
    "                \n",
    "    print('Percentage breakdown added')\n",
    "    return df\n",
    "\n",
    "# Add columns for absolute and % ranks\n",
    "def add_ranks(df):\n",
    "    \n",
    "    # Iterate through columns\n",
    "    for column in df:\n",
    "\n",
    "        # Generate index for rank columns\n",
    "        new_col = list(column)\n",
    "        new_col[0] += '_rank'\n",
    "        \n",
    "        # Add rank column for each data column\n",
    "        df[tuple(new_col)] = df[column].rank(method='min', ascending=False, na_option='keep')\n",
    "        \n",
    "    # Sort columns\n",
    "    df = df.sort_index(axis=1)\n",
    "    \n",
    "    print('Rankings added')\n",
    "    return df\n",
    "\n",
    "# Output observations for all areas (can include a maximum of one wildcard '*')\n",
    "def export_csv(dataset, obs, settings):\n",
    "    in_path = 'processed'\n",
    "    out_path = 'final/csv'\n",
    "    filename = ''\n",
    "    geography = settings['geocodes']['output']['geography']\n",
    "    geography_yr = settings['geocodes']['output']['year']\n",
    "    \n",
    "    if obs.count('*') > 1:\n",
    "        print('Too many wildcards. The maximum is 1.')\n",
    "        return None\n",
    "    \n",
    "    if obs[0] == '*':\n",
    "        print('Cant have a wildcard in first position.')\n",
    "        return None\n",
    "        \n",
    "    # Load dataset\n",
    "    load_path = settings['filepath'].format(\n",
    "        path=in_path,\n",
    "        code=dataset,\n",
    "        geography=geography + geography_yr,\n",
    "        suffix='calcs'\n",
    "    )\n",
    "    df = pd.read_parquet(load_path)\n",
    "    \n",
    "    # Get columns\n",
    "    cols = df.columns\n",
    "    years = cols.unique(level='year').values.tolist()\n",
    "    codes = cols.unique(level='code').values.tolist()\n",
    "    \n",
    "    # Create output directory if needed\n",
    "    if not os.path.exists(out_path):\n",
    "        os.mkdir(out_path)\n",
    "    \n",
    "    # Output a timeseries\n",
    "    if obs[1] == '*':\n",
    "        # Remove 'change' column from 'years'\n",
    "        years.remove('change')\n",
    "        \n",
    "        # Filter the CSV\n",
    "        df = df.loc[:, df.columns.get_level_values(0).isin([obs[0]])]\n",
    "        df = df.loc[:, df.columns.get_level_values('year').isin(years)]\n",
    "        df = df.loc[:, df.columns.get_level_values('code').isin([obs[2]])]\n",
    "        \n",
    "        # Overwrite the column headers\n",
    "        df.columns = years\n",
    "        \n",
    "        # Set filename & out_path\n",
    "        filename = dataset + '-' + obs[0] + '-' + obs[2]\n",
    "        out_path += '/series'\n",
    "    \n",
    "    # Output a breakdown for one variable\n",
    "    elif obs[2] == '*':\n",
    "        # Remove 'all' column from 'codes' for multi-value variable\n",
    "        if len(codes) > 1:\n",
    "            codes.remove('all')\n",
    "        \n",
    "        # Filter the dataframe\n",
    "        df = df.loc[:, df.columns.get_level_values(0).isin([obs[0]])]\n",
    "        df = df.loc[:, df.columns.get_level_values('year').isin([obs[1]])]\n",
    "        df = df.loc[:, df.columns.get_level_values('code').isin(codes)]\n",
    "        \n",
    "        # Overwrite the column headers\n",
    "        df.columns = codes\n",
    "        \n",
    "        # Set filename & out_path\n",
    "        filename = dataset + '-' + obs[0] + '-' + obs[1]\n",
    "        out_path += '/variable'\n",
    "    \n",
    "    # Output a single observation\n",
    "    else:\n",
    "        # Filter the dataframe\n",
    "        df = df.loc[:, df.columns.get_level_values(0).isin([obs[0]])]\n",
    "        df = df.loc[:, df.columns.get_level_values('year').isin([obs[1]])]\n",
    "        df = df.loc[:, df.columns.get_level_values('code').isin([obs[2]])]\n",
    "        \n",
    "        # Overwrite the column headers\n",
    "        df.columns = ['value']\n",
    "        \n",
    "        # Set filename & out_path\n",
    "        filename = dataset + '-' + obs[0] + '-' + obs[1] + '-' + obs[2]\n",
    "        out_path += '/single'\n",
    "            \n",
    "    # Create output directory if needed\n",
    "    if not os.path.exists(out_path):\n",
    "        os.mkdir(out_path)\n",
    "    \n",
    "    save_path = settings['flatpath'].format(\n",
    "        path=out_path,\n",
    "        geography=geography + geography_yr,\n",
    "        filename=filename\n",
    "    )\n",
    "    df.to_csv(save_path)\n",
    "    print('CSV data exported ' + save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a dataset\n",
    "The cells below should be called sequentially to build the sample dataset. Note that data from each interim step is saved to the local directory. This is for error-checking, and also to avoid having to reload data from the API in order to re-run the subsequent steps. \n",
    "1. To download and consistently map the dimensions of the source data from each census.\n",
    "2. To combine the data together into a single Tidy Data Parquet file.\n",
    "3. To pivot the data into a Pandas MultiIndex DataFrame (which can be processed like a multi-dimensional dataset).\n",
    "4. To add calculated fields for percentage breakdowns and rankings.\n",
    "5. To add parent geography codes and names, and generate JSON data files for each geography."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.nomisweb.co.uk/api/v01/dataset/NM_1634_1.data.csv?date=latest&geography=TYPE464&cell=1,2&measures=20100&select=geography_code,date_name,cell,obs_value&uid=0x3cfb19ead752b37bb90da0eb3a0fe78baa9fa055&recordlimit=250000&recordoffset=0\n",
      "Loaded population 2001 segment 1/1\n",
      "Combined + mapped raw/population_lad2015_2001.parquet\n",
      "http://www.nomisweb.co.uk/api/v01/dataset/NM_144_1.data.csv?date=latest&geography=TYPE464&cell=1,2&measures=20100&select=geography_code,date_name,cell,obs_value&uid=0x3cfb19ead752b37bb90da0eb3a0fe78baa9fa055&recordlimit=250000&recordoffset=0&rural_urban=0\n",
      "Loaded population 2011 segment 1/1\n",
      "Combined + mapped raw/population_lad2015_2011.parquet\n",
      "http://www.nomisweb.co.uk/api/v01/dataset/NM_1637_1.data.csv?date=latest&geography=TYPE464&c_age=1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81&measures=20100&select=geography_code,date_name,c_age,obs_value&uid=0x3cfb19ead752b37bb90da0eb3a0fe78baa9fa055&recordlimit=250000&recordoffset=0\n",
      "Loaded age 2001 segment 1/1\n",
      "Combined + mapped raw/age_lad2015_2001.parquet\n",
      "http://www.nomisweb.co.uk/api/v01/dataset/NM_503_1.data.csv?date=latest&geography=TYPE464&c_age=1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101&measures=20100&select=geography_code,date_name,c_age,obs_value&uid=0x3cfb19ead752b37bb90da0eb3a0fe78baa9fa055&recordlimit=250000&recordoffset=0&rural_urban=0\n",
      "Loaded age 2011 segment 1/1\n",
      "Combined + mapped raw/age_lad2015_2011.parquet\n",
      "http://www.nomisweb.co.uk/api/v01/dataset/NM_1606_1.data.csv?date=latest&geography=TYPE464&c_ethpuk11=100,200,8,9,10,11,400,15,16&measures=20100&select=geography_code,date_name,c_ethpuk11,obs_value&uid=0x3cfb19ead752b37bb90da0eb3a0fe78baa9fa055&recordlimit=250000&recordoffset=0\n",
      "Loaded ethnicity 2001 segment 1/1\n",
      "Combined + mapped raw/ethnicity_lad2015_2001.parquet\n",
      "http://www.nomisweb.co.uk/api/v01/dataset/NM_608_1.data.csv?date=latest&geography=TYPE464&cell=100,200,300,400,500&measures=20100&select=geography_code,date_name,cell,obs_value&uid=0x3cfb19ead752b37bb90da0eb3a0fe78baa9fa055&recordlimit=250000&recordoffset=0&rural_urban=0\n",
      "Loaded ethnicity 2011 segment 1/1\n",
      "Combined + mapped raw/ethnicity_lad2015_2011.parquet\n",
      "http://www.nomisweb.co.uk/api/v01/dataset/NM_1645_1.data.csv?date=latest&geography=TYPE464&c_health=1,2,3&measures=20100&select=geography_code,date_name,c_health,obs_value&uid=0x3cfb19ead752b37bb90da0eb3a0fe78baa9fa055&recordlimit=250000&recordoffset=0\n",
      "Loaded health 2001 segment 1/1\n",
      "Combined + mapped raw/health_lad2015_2001.parquet\n",
      "http://www.nomisweb.co.uk/api/v01/dataset/NM_531_1.data.csv?date=latest&geography=TYPE464&c_health=1,2,3,4,5&measures=20100&select=geography_code,date_name,c_health,obs_value&uid=0x3cfb19ead752b37bb90da0eb3a0fe78baa9fa055&recordlimit=250000&recordoffset=0&rural_urban=0\n",
      "Loaded health 2011 segment 1/1\n",
      "Combined + mapped raw/health_lad2015_2011.parquet\n",
      "http://www.nomisweb.co.uk/api/v01/dataset/NM_1651_1.data.csv?date=latest&geography=TYPE464&c_ecopuk11=2,5,8,11,12,13&measures=20100&select=geography_code,date_name,c_ecopuk11,obs_value&uid=0x3cfb19ead752b37bb90da0eb3a0fe78baa9fa055&recordlimit=250000&recordoffset=0\n",
      "Loaded economic 2001 segment 1/1\n",
      "Combined + mapped raw/economic_lad2015_2001.parquet\n",
      "http://www.nomisweb.co.uk/api/v01/dataset/NM_556_1.data.csv?date=latest&geography=TYPE464&cell=2,3,4,5,6,7,8,9,10&measures=20100&select=geography_code,date_name,cell,obs_value&uid=0x3cfb19ead752b37bb90da0eb3a0fe78baa9fa055&recordlimit=250000&recordoffset=0&rural_urban=0\n",
      "Loaded economic 2011 segment 1/1\n",
      "Combined + mapped raw/economic_lad2015_2011.parquet\n",
      "http://www.nomisweb.co.uk/api/v01/dataset/NM_1659_1.data.csv?date=latest&geography=TYPE464&transport_powpew11=1,2,3,4,5,6,7,8,9,10,11&measures=20100&select=geography_code,date_name,transport_powpew11,obs_value&uid=0x3cfb19ead752b37bb90da0eb3a0fe78baa9fa055&recordlimit=250000&recordoffset=0\n",
      "Loaded travel 2001 segment 1/1\n",
      "Combined + mapped raw/travel_lad2015_2001.parquet\n",
      "http://www.nomisweb.co.uk/api/v01/dataset/NM_568_1.data.csv?date=latest&geography=TYPE464&cell=1,2,3,4,5,6,7,8,9,10,11&measures=20100&select=geography_code,date_name,cell,obs_value&uid=0x3cfb19ead752b37bb90da0eb3a0fe78baa9fa055&recordlimit=250000&recordoffset=0&rural_urban=0\n",
      "Loaded travel 2011 segment 1/1\n",
      "Combined + mapped raw/travel_lad2015_2011.parquet\n",
      "http://www.nomisweb.co.uk/api/v01/dataset/NM_1680_1.data.csv?date=latest&geography=TYPE464&c_tenhuk11=1,4,5,8,13&measures=20100&select=geography_code,date_name,c_tenhuk11,obs_value&uid=0x3cfb19ead752b37bb90da0eb3a0fe78baa9fa055&recordlimit=250000&recordoffset=0\n",
      "Loaded tenure 2001 segment 1/1\n",
      "Combined + mapped raw/tenure_lad2015_2001.parquet\n",
      "http://www.nomisweb.co.uk/api/v01/dataset/NM_537_1.data.csv?date=latest&geography=TYPE464&c_tenhuk11=1,4,5,8,13&measures=20100&select=geography_code,date_name,c_tenhuk11,obs_value&uid=0x3cfb19ead752b37bb90da0eb3a0fe78baa9fa055&recordlimit=250000&recordoffset=0&rural_urban=0\n",
      "Loaded tenure 2011 segment 1/1\n",
      "Combined + mapped raw/tenure_lad2015_2011.parquet\n"
     ]
    }
   ],
   "source": [
    "# Get all sources for all datasets (creates one Parquet per dataset & per census year)\n",
    "# Only run this cell ONE TIME for each source geography\n",
    "# Output Area-level data takes a LONG TIME to load, so don't waste your time doing it more than once\n",
    "for dataset in DATASETS:\n",
    "    get_sources(dataset, SETTINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated raw/population_lad2021_2001.parquet\n",
      "Aggregated raw/population_lad2021_2011.parquet\n",
      "Aggregated raw/age_lad2021_2001.parquet\n",
      "Aggregated raw/age_lad2021_2011.parquet\n",
      "Aggregated raw/ethnicity_lad2021_2001.parquet\n",
      "Aggregated raw/ethnicity_lad2021_2011.parquet\n",
      "Aggregated raw/health_lad2021_2001.parquet\n",
      "Aggregated raw/health_lad2021_2011.parquet\n",
      "Aggregated raw/economic_lad2021_2001.parquet\n",
      "Aggregated raw/economic_lad2021_2011.parquet\n",
      "Aggregated raw/travel_lad2021_2001.parquet\n",
      "Aggregated raw/travel_lad2021_2011.parquet\n",
      "Aggregated raw/tenure_lad2021_2001.parquet\n",
      "Aggregated raw/tenure_lad2021_2011.parquet\n"
     ]
    }
   ],
   "source": [
    "# Group (aggregate) data to output geography using lookups\n",
    "# If your source and output geography codes are the same this function will automatically skip over\n",
    "for dataset in DATASETS:\n",
    "    group_data(dataset, SETTINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined processed/population_lad2021_tidy.parquet\n",
      "Combined processed/age_lad2021_tidy.parquet\n",
      "Combined processed/ethnicity_lad2021_tidy.parquet\n",
      "Combined processed/health_lad2021_tidy.parquet\n",
      "Combined processed/economic_lad2021_tidy.parquet\n",
      "Combined processed/travel_lad2021_tidy.parquet\n",
      "Combined processed/tenure_lad2021_tidy.parquet\n"
     ]
    }
   ],
   "source": [
    "# Combine years for all datasets (creates one Tidy Data CSV per dataset)\n",
    "for dataset in DATASETS:\n",
    "    combine_data(dataset, SETTINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data pivoted processed/population_lad2021_pivot.parquet\n",
      "Data pivoted processed/age_lad2021_pivot.parquet\n",
      "Data pivoted processed/ethnicity_lad2021_pivot.parquet\n",
      "Data pivoted processed/health_lad2021_pivot.parquet\n",
      "Data pivoted processed/economic_lad2021_pivot.parquet\n",
      "Data pivoted processed/travel_lad2021_pivot.parquet\n",
      "Data pivoted processed/tenure_lad2021_pivot.parquet\n"
     ]
    }
   ],
   "source": [
    "# Pivot all datasets (pivots data into multi-index columns)\n",
    "for dataset in DATASETS:\n",
    "    pivot_data(dataset, SETTINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating density\n",
      "Totals added\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barcla\\Anaconda3\\envs\\geo-env\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3343: DtypeWarning: Columns (2) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proxy data processed/density_lad2021_pivot.parquet\n",
      "Mapping age10yr\n",
      "Proxy data processed/age10yr_lad2021_pivot.parquet\n",
      "Calculating median age\n",
      "Totals added\n",
      "Proxy data processed/agemed_lad2021_pivot.parquet\n"
     ]
    }
   ],
   "source": [
    "# Add proxy (derived) datasets (in pivoted format, to match above)\n",
    "add_proxy_data(PROXY_DATASETS, SETTINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totals added\n",
      "% change added\n",
      "Percentage breakdown added\n",
      "Rankings added\n",
      "Calculated fields added processed/population_lad2021_calcs.parquet\n",
      "Totals added\n",
      "% change added\n",
      "Percentage breakdown added\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barcla\\Anaconda3\\envs\\geo-env\\lib\\site-packages\\ipykernel_launcher.py:478: UserWarning: The DataFrame has column names of mixed type. They will be converted to strings and not roundtrip correctly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rankings added\n",
      "Calculated fields added processed/ethnicity_lad2021_calcs.parquet\n",
      "Totals added\n",
      "% change added\n",
      "Percentage breakdown added\n",
      "Rankings added\n",
      "Calculated fields added processed/health_lad2021_calcs.parquet\n",
      "Totals added\n",
      "% change added\n",
      "Percentage breakdown added\n",
      "Rankings added\n",
      "Calculated fields added processed/economic_lad2021_calcs.parquet\n",
      "Totals added\n",
      "% change added\n",
      "Percentage breakdown added\n",
      "Rankings added\n",
      "Calculated fields added processed/travel_lad2021_calcs.parquet\n",
      "Totals added\n",
      "% change added\n",
      "Percentage breakdown added\n",
      "Rankings added\n",
      "Calculated fields added processed/tenure_lad2021_calcs.parquet\n",
      "% change added\n",
      "Rankings added\n",
      "Calculated fields added processed/density_lad2021_calcs.parquet\n",
      "Totals added\n",
      "% change added\n",
      "Percentage breakdown added\n",
      "Rankings added\n",
      "Calculated fields added processed/age10yr_lad2021_calcs.parquet\n",
      "% change added\n",
      "Rankings added\n",
      "Calculated fields added processed/agemed_lad2021_calcs.parquet\n"
     ]
    }
   ],
   "source": [
    "# Perform calculations on all datasets (incl. proxy datasets) for %change, %breakdown and rankings\n",
    "for dataset in ALL_DATASETS:\n",
    "    add_calcs(dataset, SETTINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barcla\\Anaconda3\\envs\\geo-env\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3263: DtypeWarning: Columns (2) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lookups generated\n",
      "Datasets loaded\n",
      "Datasets merged processed/all_lad2021_merged.parquet\n",
      "Deciles data saved to JSON file final/json/quantiles/deciles_lad2021.json\n",
      "Quintiles data saved to JSON file final/json/quantiles/quintiles_lad2021.json\n",
      "Quartiles data saved to JSON file final/json/quantiles/quartiles_lad2021.json\n",
      "Geographies saved to individual JSON files final/{geocode}.json\n"
     ]
    }
   ],
   "source": [
    "export_data(ALL_DATASETS, SETTINGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export CSV Data\n",
    "This section of the code allows data to be exported in CSV files covering all geographies. Edit the parameters for the  \"export_csv\" function in the cell below, as follows:\n",
    "1. Select the dataset code (eg. 'population', 'age10yr', 'medage', 'density')\n",
    "2. Select the filter pattern ['val', 'year', 'variable']\n",
    "\n",
    "Options for each parameter:\n",
    "* 'val' can be 'value', 'perc', 'value_rank' or 'perc_rank'\n",
    "* 'year' can be '2001', '2011' or 'change' for % change from 2001 to 2011\n",
    "* 'variable' options depend on the dataset. Select 'all' to return the sum total\n",
    "* one of 'year' or 'variable' can be a wildcard '*' to create a timeseries or a breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV data exported final/csv/variable/oa2011_travel-value-2011.csv\n"
     ]
    }
   ],
   "source": [
    "# Export a filtered CSV file covering all geographies\n",
    "export_csv('travel', ['value', '2011', '*'], SETTINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
